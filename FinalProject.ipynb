{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FinalProject.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9cFKLdKkWI3",
        "colab_type": "code",
        "outputId": "376e17a2-304a-4263-87f2-0a93d8daeef0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "source": [
        "##Opening Data And saving\n",
        "\n",
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!ls '/content/drive/Team Drives/Deep Learning/FinalProject/'\n",
        "myDrive = '/content/drive/Team Drives/Deep Learning/FinalProject/'\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "'Còpia de FinalProject.ipynb'\t     FinalProject.ipynb\n",
            "'Còpia de FinalProjectJORDI.ipynb'   image_49.png\n",
            " data\t\t\t\t     models\n",
            "'Face emotion recognition.gdoc'     'Presentació sense títol.gslides'\n",
            "'Face emotion recognition.pdf'\t     Source.gdoc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kTDoaQiJ0tg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Libraries needed to run this file.\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision import datasets, models, transforms\n",
        "import torch.nn as nn\n",
        "from torchvision.utils import save_image\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim\n",
        "import os\n",
        "from PIL import Image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFectLfdJ3Eh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Creates a list that contains the possible labels for the images (both train and test).\n",
        "labelList = ['angry', 'disgusted', 'fearful', 'happy', 'neutral', 'sad', 'surprised']\n",
        "#Defines cuda device for running this file if it is available.\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#Loads data from our dataset. Can be used for train and test loaders.\n",
        "class emotionLoader(torch.utils.data.Dataset):\n",
        "    #Loads all the images and its corresponding label.\n",
        "    def __init__(self,dataDir = './data/train/',transform = None):\n",
        "        #Initialize the data and label list\n",
        "        self.labels = []\n",
        "        self.data = []\n",
        "        \n",
        "        #List all the images of the given folder and sort them.\n",
        "        listImage = os.listdir(dataDir)\n",
        "        listImage = sorted(listImage)\n",
        "        \n",
        "        #For loop through the label list.\n",
        "        for i in range( len(labelList) ):\n",
        "            print(labelList[i])\n",
        "            #Creates the path of each folder (angry, sad, neutral, ...).\n",
        "            path = dataDir + labelList[i] + '/'\n",
        "            #List the images inside the current folder.\n",
        "            listImage = os.listdir(path)\n",
        "\n",
        "            #For loop through the images of a folder.\n",
        "            for face in listImage:\n",
        "                #Read the data using PIL and appends it to data list.\n",
        "                self.data.append(Image.open(path+face))\n",
        "                #Also appends the label of the current image.\n",
        "                self.labels.append(torch.FloatTensor([i]))\n",
        "        \n",
        "        self.transform = transform\n",
        "        \n",
        "    #Given and index, returns the image and label located at this index.\n",
        "    def __getitem__(self, index):\n",
        "        #Gets the image and the label located at the given index.\n",
        "        data = self.data[index]\n",
        "        lbl = self.labels[index]\n",
        "        \n",
        "        #If there are some transformations apply them to the image.\n",
        "        if self.transform is not None : \n",
        "            data = self.transform(data)\n",
        "        \n",
        "        #Return both image and label.\n",
        "        return data,lbl\n",
        "    \n",
        "    #Returns the size of data loaded.\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhqwsfDqs4wl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Our network.\n",
        "class Net(nn.Module):\n",
        "    \n",
        "    #Define all the layers involved and the number of classes.\n",
        "    def __init__(self, num_classes=7):\n",
        "        super(Net, self).__init__()     \n",
        "        #As label list has length 7, our network will have 7 classes.\n",
        "        self.classes = num_classes\n",
        "        #First liner layer. In features = 64*23*23 and out features = 64.\n",
        "        self.in1 = nn.Linear(64*23*23, 64)\n",
        "        #Second linear layer. In features = out features linear layer 1 and out layer = number of clases.\n",
        "        self.in2 = nn.Linear(64, 7)\n",
        "        #First conv. layer. In channels = 1, out channels = 32 and kernel size of 3x3.\n",
        "        self.l1 = nn.Conv2d(1, 32, 3)\n",
        "        #Second conv.layer. In channel = out channels conv. layer 1, out channels = 64 and kernel size of 1x1.\n",
        "        self.l2 = nn.Conv2d(32, 64, 1)      \n",
        "        \n",
        "        #Relu activation function.\n",
        "        self.relu =  nn.ReLU(True)\n",
        "        #Max pooling with kernel size of 2x2.\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        #Dropout with probability of an element to be zeroed equals to 0.25.\n",
        "        self.drop = nn.Dropout(0.25)\n",
        "        #Batch normalization with  64 in features.\n",
        "        self.batchNorm = nn.BatchNorm2d(64)\n",
        "\n",
        "    #Forward function.\n",
        "    def forward(self, x):\n",
        "        #Returns the result of passing the image x through the first convolutional layer.\n",
        "        out = self.l1(x)\n",
        "        #Apply relu activation function to the previous result.\n",
        "        out = self.relu(out)\n",
        "        #After that, pass the above result through the second convolutional layer.\n",
        "        out = self.l2(out)\n",
        "        #Again, apply relu activation function to that result.\n",
        "        out = self.relu(out)\n",
        "        #Apply batch normalization, max pooling and dropout.\n",
        "        out = self.batchNorm(out)\n",
        "        out = self.pool(out)\n",
        "        out = self.drop(out)\n",
        "        #Reshape the result in order to remove one dimension of it.\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        #Finally, pass the result value through first linear layer and then through second linear layer.\n",
        "        out = self.in1(out)\n",
        "        out = self.in2(out)\n",
        "\n",
        "        #Return the result.\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0PhAxgsIXXO",
        "colab_type": "code",
        "outputId": "58d6550f-b2a1-4b29-efe8-6bd04e50e8d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "#Transformation that will be applied to each image.\n",
        "tr = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([.5], [.5])\n",
        "        ])\n",
        "\n",
        "#Defines the batch size used in both train and test loaders.\n",
        "batch_size = 64\n",
        "\n",
        "#Defines train loader.\n",
        "train = emotionLoader(myDrive + 'data/train/', tr)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train,\n",
        "                                               batch_size=batch_size, \n",
        "                                               shuffle=True)\n",
        "#Defines test loader.\n",
        "test = emotionLoader(myDrive + 'data/test/', tr)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test,\n",
        "                                               batch_size=batch_size, \n",
        "                                               shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "angry\n",
            "disgusted\n",
            "fearful\n",
            "happy\n",
            "neutral\n",
            "sad\n",
            "surprised\n",
            "angry\n",
            "disgusted\n",
            "fearful\n",
            "happy\n",
            "neutral\n",
            "sad\n",
            "surprised\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01uAmDrqy4zP",
        "colab_type": "code",
        "outputId": "bc2cd786-40bc-4bf3-8c7b-1acac5237ee6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1861
        }
      },
      "source": [
        "#The training will have 50 epochs.\n",
        "num_epochs = 50\n",
        "\n",
        "#Defines the neural network we will use and loads a model.\n",
        "CNN = Net()\n",
        "CNN.load_state_dict(torch.load(myDrive + 'models/VGG_Face_torch.pth'), strict=False)\n",
        "CNN = CNN.cuda()\n",
        "\n",
        "#Defines the learning rate, the criterion and the optimizer.\n",
        "learning_rate = .01\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(CNN.parameters(),lr = learning_rate)\n",
        "\n",
        "#Train the model.\n",
        "if True :\n",
        "    \n",
        "    print('training')\n",
        "    total_step = len(train_loader)\n",
        "    \n",
        "    #For loop for epochs.\n",
        "    for epoch in range(num_epochs):\n",
        "        #For loop through train loader.\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            \n",
        "            #The images and labels are send to cuda.\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            #Forward pass\n",
        "            outputs = CNN(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            #Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if (i+1) % 16 == 0:\n",
        "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                       .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:443: UserWarning: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([64, 7])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/50], Step [16/37], Loss: 135.5347\n",
            "Epoch [1/50], Step [32/37], Loss: 26.9488\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:443: UserWarning: Using a target size (torch.Size([41, 1])) that is different to the input size (torch.Size([41, 7])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [2/50], Step [16/37], Loss: 8.4128\n",
            "Epoch [2/50], Step [32/37], Loss: 5.8300\n",
            "Epoch [3/50], Step [16/37], Loss: 5.9808\n",
            "Epoch [3/50], Step [32/37], Loss: 5.3439\n",
            "Epoch [4/50], Step [16/37], Loss: 4.7551\n",
            "Epoch [4/50], Step [32/37], Loss: 5.2715\n",
            "Epoch [5/50], Step [16/37], Loss: 4.9437\n",
            "Epoch [5/50], Step [32/37], Loss: 5.5526\n",
            "Epoch [6/50], Step [16/37], Loss: 4.3204\n",
            "Epoch [6/50], Step [32/37], Loss: 4.5178\n",
            "Epoch [7/50], Step [16/37], Loss: 4.7856\n",
            "Epoch [7/50], Step [32/37], Loss: 4.0562\n",
            "Epoch [8/50], Step [16/37], Loss: 4.0817\n",
            "Epoch [8/50], Step [32/37], Loss: 3.2570\n",
            "Epoch [9/50], Step [16/37], Loss: 3.9865\n",
            "Epoch [9/50], Step [32/37], Loss: 5.0293\n",
            "Epoch [10/50], Step [16/37], Loss: 4.5078\n",
            "Epoch [10/50], Step [32/37], Loss: 3.5477\n",
            "Epoch [11/50], Step [16/37], Loss: 3.6817\n",
            "Epoch [11/50], Step [32/37], Loss: 3.1213\n",
            "Epoch [12/50], Step [16/37], Loss: 3.4172\n",
            "Epoch [12/50], Step [32/37], Loss: 3.8474\n",
            "Epoch [13/50], Step [16/37], Loss: 3.5885\n",
            "Epoch [13/50], Step [32/37], Loss: 3.9218\n",
            "Epoch [14/50], Step [16/37], Loss: 4.5887\n",
            "Epoch [14/50], Step [32/37], Loss: 3.4433\n",
            "Epoch [15/50], Step [16/37], Loss: 2.8031\n",
            "Epoch [15/50], Step [32/37], Loss: 3.4758\n",
            "Epoch [16/50], Step [16/37], Loss: 3.3255\n",
            "Epoch [16/50], Step [32/37], Loss: 3.0420\n",
            "Epoch [17/50], Step [16/37], Loss: 2.9856\n",
            "Epoch [17/50], Step [32/37], Loss: 3.0304\n",
            "Epoch [18/50], Step [16/37], Loss: 2.3640\n",
            "Epoch [18/50], Step [32/37], Loss: 3.3830\n",
            "Epoch [19/50], Step [16/37], Loss: 2.7688\n",
            "Epoch [19/50], Step [32/37], Loss: 3.2277\n",
            "Epoch [20/50], Step [16/37], Loss: 2.2123\n",
            "Epoch [20/50], Step [32/37], Loss: 2.5285\n",
            "Epoch [21/50], Step [16/37], Loss: 2.0525\n",
            "Epoch [21/50], Step [32/37], Loss: 2.0945\n",
            "Epoch [22/50], Step [16/37], Loss: 1.7451\n",
            "Epoch [22/50], Step [32/37], Loss: 2.2080\n",
            "Epoch [23/50], Step [16/37], Loss: 2.2347\n",
            "Epoch [23/50], Step [32/37], Loss: 1.7205\n",
            "Epoch [24/50], Step [16/37], Loss: 2.1276\n",
            "Epoch [24/50], Step [32/37], Loss: 1.8492\n",
            "Epoch [25/50], Step [16/37], Loss: 1.8101\n",
            "Epoch [25/50], Step [32/37], Loss: 1.6497\n",
            "Epoch [26/50], Step [16/37], Loss: 1.6690\n",
            "Epoch [26/50], Step [32/37], Loss: 2.5162\n",
            "Epoch [27/50], Step [16/37], Loss: 1.5553\n",
            "Epoch [27/50], Step [32/37], Loss: 2.4301\n",
            "Epoch [28/50], Step [16/37], Loss: 1.6894\n",
            "Epoch [28/50], Step [32/37], Loss: 1.8246\n",
            "Epoch [29/50], Step [16/37], Loss: 1.7520\n",
            "Epoch [29/50], Step [32/37], Loss: 2.1882\n",
            "Epoch [30/50], Step [16/37], Loss: 1.3833\n",
            "Epoch [30/50], Step [32/37], Loss: 2.1156\n",
            "Epoch [31/50], Step [16/37], Loss: 1.7427\n",
            "Epoch [31/50], Step [32/37], Loss: 2.0449\n",
            "Epoch [32/50], Step [16/37], Loss: 1.4794\n",
            "Epoch [32/50], Step [32/37], Loss: 2.1046\n",
            "Epoch [33/50], Step [16/37], Loss: 2.0442\n",
            "Epoch [33/50], Step [32/37], Loss: 2.0360\n",
            "Epoch [34/50], Step [16/37], Loss: 1.6064\n",
            "Epoch [34/50], Step [32/37], Loss: 1.4816\n",
            "Epoch [35/50], Step [16/37], Loss: 1.2467\n",
            "Epoch [35/50], Step [32/37], Loss: 1.2903\n",
            "Epoch [36/50], Step [16/37], Loss: 1.3994\n",
            "Epoch [36/50], Step [32/37], Loss: 1.3050\n",
            "Epoch [37/50], Step [16/37], Loss: 1.2021\n",
            "Epoch [37/50], Step [32/37], Loss: 1.6667\n",
            "Epoch [38/50], Step [16/37], Loss: 1.4216\n",
            "Epoch [38/50], Step [32/37], Loss: 1.8898\n",
            "Epoch [39/50], Step [16/37], Loss: 1.1248\n",
            "Epoch [39/50], Step [32/37], Loss: 1.5146\n",
            "Epoch [40/50], Step [16/37], Loss: 1.4696\n",
            "Epoch [40/50], Step [32/37], Loss: 1.3512\n",
            "Epoch [41/50], Step [16/37], Loss: 1.2706\n",
            "Epoch [41/50], Step [32/37], Loss: 1.7767\n",
            "Epoch [42/50], Step [16/37], Loss: 1.1217\n",
            "Epoch [42/50], Step [32/37], Loss: 1.6666\n",
            "Epoch [43/50], Step [16/37], Loss: 1.1304\n",
            "Epoch [43/50], Step [32/37], Loss: 1.0662\n",
            "Epoch [44/50], Step [16/37], Loss: 1.0347\n",
            "Epoch [44/50], Step [32/37], Loss: 1.2248\n",
            "Epoch [45/50], Step [16/37], Loss: 1.5272\n",
            "Epoch [45/50], Step [32/37], Loss: 1.0355\n",
            "Epoch [46/50], Step [16/37], Loss: 1.2130\n",
            "Epoch [46/50], Step [32/37], Loss: 1.4874\n",
            "Epoch [47/50], Step [16/37], Loss: 0.9658\n",
            "Epoch [47/50], Step [32/37], Loss: 1.1504\n",
            "Epoch [48/50], Step [16/37], Loss: 1.0161\n",
            "Epoch [48/50], Step [32/37], Loss: 1.2210\n",
            "Epoch [49/50], Step [16/37], Loss: 0.8756\n",
            "Epoch [49/50], Step [32/37], Loss: 0.8798\n",
            "Epoch [50/50], Step [16/37], Loss: 1.0959\n",
            "Epoch [50/50], Step [32/37], Loss: 0.9393\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIfrpQBrQcd0",
        "colab_type": "code",
        "outputId": "7718e778-5fea-48b1-b400-a347646286a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "source": [
        "#Modifies the image size as it is modified by the neural network.\n",
        "def to_img(x):\n",
        "    x = 0.5 * (x + 1)\n",
        "    x = x.clamp(0, 1)\n",
        "    x = x.view(x.size(0), 1, 48, 48)\n",
        "    return x\n",
        "\n",
        "#Test the model.\n",
        "if True :\n",
        "    \n",
        "    #Eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance).\n",
        "    CNN.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        \n",
        "        #For loop through test loader.\n",
        "        for i, (images, labels) in enumerate(test_loader):\n",
        "            #Send both images and labels to cuda in order to perform faster this operations.\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            #Return the result of passing the images through the neural network.\n",
        "            outputs = CNN(images)\n",
        "            \n",
        "            #Computes values related to accuracy.\n",
        "            total += labels.size(0)\n",
        "            for line in outputs:\n",
        "                index = (line == torch.max(line)).nonzero()\n",
        "                correct += (index == int(labels[i].item())).sum().item()\n",
        "            \n",
        "            #Shows an example of our model.\n",
        "            if i%60 == 0:\n",
        "                pic = to_img(images[i].cpu().data)\n",
        "                imgName = myDrive+'image_{}.png'.format(epoch)\n",
        "                save_image(pic, imgName)\n",
        "                plt.imshow(Image.open(imgName))\n",
        "                print('Predicted label = ', index.item())\n",
        "                print('Real label = ', int(labels[i].item()))\n",
        "            \n",
        "        print('Test Accuracy of the model on the test images: {} %'.format(100 * (correct / total) ) )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted label =  4\n",
            "Real label =  4\n",
            "Test Accuracy of the model on the test images: 18.142857142857142 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxIZvTIX8P6WnjQytc/NdAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gq4_p-YelmZS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}